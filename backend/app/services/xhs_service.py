"""
å°çº¢ä¹¦æœåŠ¡å±‚ - æ—…æ¸¸åšä¸»ç¬”è®°è®¢é˜…ï¼ˆå¢å¼ºç‰ˆï¼‰
"""

import httpx
import asyncio
import re
import urllib.parse
import feedparser
from typing import Dict, Any, List, Optional
from datetime import datetime
from app.config import get_settings

settings = get_settings()

# ============================================================
# æ—…æ¸¸åšä¸»åˆ—è¡¨
# ============================================================
# å¦‚ä½•è·å–åšä¸» IDï¼š
#   1. æ‰“å¼€å°çº¢ä¹¦ App æˆ–ç½‘é¡µç‰ˆ (xiaohongshu.com)
#   2. è¿›å…¥åšä¸»ä¸»é¡µï¼ŒæŸ¥çœ‹ URLï¼šxiaohongshu.com/user/profile/XXXXXX
#   3. URL ä¸­ profile/ åé¢çš„ 24 ä½å­—ç¬¦ä¸²å³ä¸ºåšä¸» ID
#   4. ä¾‹å¦‚ï¼šhttps://www.xiaohongshu.com/user/profile/593032945e87e77791e03696
#      åšä¸» ID ä¸ºï¼š593032945e87e77791e03696
# ============================================================
TRAVEL_BLOGGERS = [
    # å·²éªŒè¯æœ‰æ•ˆçš„åšä¸»
    {"id": "593032945e87e77791e03696", "name": "å°å®‡è‡è‡", "tags": ["æ—…æ¸¸", "æ”»ç•¥"]},
     # å¬‰æ¸¸: å…¨ç½‘æœ€é¡¶çš„æœºé…’å¡æ”»ç•¥ï¼Œéå¸¸ä¸“ä¸š
    {"id": "5aec57f04eacab43557f7b77", "name": "å¬‰æ¸¸å°åŠ©ç†", "tags": ["æœºé…’æ”»ç•¥", "ä¸“ä¸š"]},
    # å°å¢¨ä¸é˜¿çŒ´: æ‘„å½±ä¸æ”»ç•¥ç»“åˆï¼Œé€‚åˆæƒ…ä¾£/å¹´è½»äººç¾¤ä½“
    {"id": "52f59215b4c4d66b2eafa21d", "name": "å°å¢¨ä¸é˜¿çŒ´", "tags": ["æ‘„å½±", "æƒ…ä¾£æ¸¸"]},
    # è¿™é‡Œæ˜¯æ–°ç–†: ä¸“æ³¨æ–°ç–†æ—…æ¸¸ï¼Œéå¸¸å‚ç›´
    {"id": "5acf498411be105586e79b4c", "name": "è¿™é‡Œæ˜¯æ–°ç–†", "tags": ["æ–°ç–†", "å‚ç›´æ”»ç•¥"]},
    # --- å°ä¼—ç§˜å¢ƒ (é¿å¼€äººæµ) ---
    # æˆ¿çªkiki: æ²»æ„ˆç³»æ–‡æ¡ˆï¼Œæ¨èçš„åœ°æ–¹æœ‰æ ¼è°ƒ
    {"id": "5bf9ff7e999837000189d106", "name": "æˆ¿çªkiki", "tags": ["æ²»æ„ˆ", "æ–‡æ¡ˆ", "å°ä¼—"]},
     # å°ğŸ‘çˆ±æºœè¾¾ï½: åŒ—äº¬æœ¬åœ°åƒå–ç©ä¹ä¸“å®¶
    {"id": "616cdf5a000000001f03a074", "name": "å°ğŸ‘çˆ±æºœè¾¾ï½", "tags": ["åŒ—äº¬", "æœ¬åœ°ç”Ÿæ´»"]},
    # å°é¹¿Lawrence: å¯¼æ¼”/æ‘„å½±å¸ˆï¼Œé«˜è´¨é‡ç¯çƒæ—…è¡Œå½±åƒ
    {"id": "5af05c664eacab116931c0d0", "name": "å°é¹¿Lawrence", "tags": ["ç¯çƒæ—…è¡Œ", "å½±åƒ", "æ‘„å½±"]},
    # Linksphotograph: é¡¶çº§é£å…‰æ‘„å½±ï¼Œæ¢ç´¢ä¸–ç•Œæå¢ƒ
    {"id": "5f0a7dfb0000000001007eaa", "name": "Linksphotograph", "tags": ["é£å…‰æ‘„å½±", "æé™æ¢ç´¢"]},
    # è´è´è´è´è´ (æ”»ç•¥ç‰ˆ): ä¸“æ³¨æ—…æ¸¸æ”»ç•¥åˆ†äº«
    {"id": "6613e7610000000003033ddc", "name": "è´è´è´è´è´ (æ”»ç•¥ç‰ˆ)", "tags": ["æ”»ç•¥", "æ‰“å¡"]},
    # æ—…è¡Œæ­å­å°çˆ±é…±: æ±Ÿæµ™æ²ªå‘¨è¾¹æ¸¸ï¼Œä¸»æ‰“é«˜æ€§ä»·æ¯”å’Œçœé’±æ”»ç•¥
    {"id": "64239daf00000000120120dd", "name": "æ—…è¡Œæ­å­å°çˆ±é…±", "tags": ["æ±Ÿæµ™æ²ª", "çœé’±æ”»ç•¥"]},
    # Edençš„ç¯çƒæ—…è¡Œï¼š29å²èŠ±33ä¸‡ç¯çƒæ—…è¡Œï¼Œå·²å»è¿‡ä¸ƒå¤§æ´²40+å›½å®¶
    {"id": "5ffd4e370000000001008dbc", "name": "Edençš„ç¯çƒæ—…è¡Œ", "tags": ["ç¯çƒæ—…è¡Œ", "æ”»ç•¥", "ç”Ÿæ´»æ–¹å¼"]},

    # æ·»åŠ æ›´å¤šåšä¸»æ—¶ï¼Œè¯·å…ˆéªŒè¯ ID æœ‰æ•ˆæ€§ï¼Œæ ¼å¼ï¼š
    # {"id": "24ä½ç”¨æˆ·ID", "name": "æ˜µç§°", "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"]},
]


def _fix_localhost_url(url: str) -> str:
    """å°† localhost æ›¿æ¢ä¸º 127.0.0.1ï¼Œé¿å… IPv6 è§£æé—®é¢˜"""
    return url.replace("localhost", "127.0.0.1")


async def check_rsshub_health() -> Dict[str, Any]:
    """æ£€æŸ¥ RSSHub æœåŠ¡å¥åº·çŠ¶æ€"""
    results = {}
    
    for url in [settings.xhs_rsshub_base_url, settings.xhs_rsshub_fallback_url]:
        fixed_url = _fix_localhost_url(url)
        try:
            async with httpx.AsyncClient(timeout=5.0, trust_env=False) as client:
                health_url = f"{fixed_url}/healthz"
                print(f"[DEBUG] å¥åº·æ£€æŸ¥: {health_url}")
                resp = await client.get(health_url)
                print(f"[DEBUG] å“åº”: {resp.status_code}")
                results[url] = {"status": "ok" if resp.status_code == 200 else "error", "code": resp.status_code}
        except Exception as e:
            print(f"[DEBUG] å¥åº·æ£€æŸ¥å¼‚å¸¸: {type(e).__name__}: {e}")
            results[url] = {"status": "error", "error": str(e)}
    
    return results


async def get_xhs_notes(keyword: str) -> Dict[str, Any]:
    """
    è·å–å°çº¢ä¹¦ç¬”è®°æ•°æ®
    ç­–ç•¥ï¼šå…ˆå°è¯•æœ¬åœ° RSSHubï¼Œå¤±è´¥ååˆ‡æ¢åˆ°å…¬å…±å®ä¾‹
    """
    rsshub_urls = [settings.xhs_rsshub_base_url, settings.xhs_rsshub_fallback_url]
    
    print(f"[XHS] å¼€å§‹è·å–ç¬”è®°ï¼Œå…³é”®è¯: {keyword}")
    
    for base_url in rsshub_urls:
        print(f"[XHS] å°è¯• RSSHub: {base_url}")
        result = await _try_get_notes(base_url, keyword)
        
        if result["status"] == "success" and result["data"]:
            print(f"[XHS] æˆåŠŸä» {base_url} è·å– {len(result['data'])} æ¡ç¬”è®°")
            return result
        
        print(f"[XHS] {base_url} å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
    
    print("[XHS] æ‰€æœ‰ RSSHub å®ä¾‹å¤±è´¥ï¼Œè¿”å›é™çº§å“åº”")
    return {
        "status": "fallback",
        "data": [],
        "search_url": _build_search_url(keyword),
        "message": "å°çº¢ä¹¦æ•°æ®æš‚æ—¶æ— æ³•è·å–ï¼Œè¯·ç‚¹å‡»é“¾æ¥ç›´æ¥æœç´¢"
    }


async def _try_get_notes(base_url: str, keyword: str) -> Dict[str, Any]:
    """å°è¯•ä»æŒ‡å®š RSSHub å®ä¾‹è·å–ç¬”è®° - æœç´¢æ‰€æœ‰åšä¸»ï¼Œä¸¥æ ¼åŒ¹é…å…³é”®è¯"""
    matched_notes = []  # åªå­˜å‚¨åŒ¹é…å…³é”®è¯çš„ç¬”è®°
    fixed_base_url = _fix_localhost_url(base_url)
    success_count = 0
    
    print(f"[DEBUG] _try_get_notes: åŸå§‹URL={base_url}, ä¿®æ­£å={fixed_base_url}")
    print(f"[DEBUG] å¼€å§‹æœç´¢å…¨éƒ¨ {len(TRAVEL_BLOGGERS)} ä¸ªåšä¸»...")
    
    # å¹¶å‘è¯·æ±‚æ‰€æœ‰åšä¸»ï¼Œæé«˜æ•ˆç‡
    async with httpx.AsyncClient(timeout=30.0, trust_env=False) as client:
        for i, blogger in enumerate(TRAVEL_BLOGGERS):
            if i > 0:
                await asyncio.sleep(0.5)  # å‡å°‘é—´éš”ï¼ŒåŠ å¿«æœç´¢
            
            url = f"{fixed_base_url}/xiaohongshu/user/{blogger['id']}/notes"
            
            try:
                print(f"[DEBUG] [{i+1}/{len(TRAVEL_BLOGGERS)}] è¯·æ±‚: {blogger['name']}")
                response = await client.get(url)
                print(f"[DEBUG] {blogger['name']}: status={response.status_code}, len={len(response.text)}")
                
                if response.status_code != 200:
                    print(f"[DEBUG] å“åº”å†…å®¹: {response.text[:200]}")
                    continue
                
                success_count += 1
                notes = _parse_rss(response.text, blogger["name"], blogger.get("tags", []))
                
                if notes:
                    # ä¸¥æ ¼åŒ¹é…ï¼šåªæ·»åŠ åŒ¹é…å…³é”®è¯çš„ç¬”è®°
                    filtered = _filter_by_keyword(notes, keyword)
                    if filtered:
                        matched_notes.extend(filtered)
                        print(f"[DEBUG] {blogger['name']}: åŒ¹é… {len(filtered)} æ¡")
                            
            except httpx.TimeoutException:
                print(f"[DEBUG] è¶…æ—¶: {blogger['name']}")
            except Exception as e:
                print(f"[DEBUG] å¼‚å¸¸ {blogger['name']}: {type(e).__name__}: {e}")
    
    print(f"[DEBUG] æœç´¢å®Œæˆ: æˆåŠŸè¯·æ±‚ {success_count}/{len(TRAVEL_BLOGGERS)} ä¸ªåšä¸», åŒ¹é… {len(matched_notes)} æ¡ç¬”è®°")
    
    # ä¸¥æ ¼åŒ¹é…ï¼šæœ‰åŒ¹é…ç»“æœæ‰è¿”å› successï¼Œå¦åˆ™è¿”å› fallback
    if matched_notes:
        matched_notes.sort(key=lambda x: x.get("liked_count", 0), reverse=True)
        return {
            "status": "success",
            "data": matched_notes[:6],
            "search_url": _build_search_url(keyword)
        }
    
    # æ— åŒ¹é…æ—¶ç›´æ¥é™çº§ï¼Œä¸è¿”å›ä¸ç›¸å…³çš„å¸–æ–‡
    return {"status": "fallback", "data": [], "search_url": _build_search_url(keyword)}


def _parse_rss(xml_content: str, author_name: str, author_tags: List[str]) -> List[Dict]:
    """è§£æ RSS XMLï¼Œæå–å®Œæ•´ç¬”è®°ä¿¡æ¯"""
    notes = []
    
    try:
        feed = feedparser.parse(xml_content)
        
        for i, entry in enumerate(feed.entries[:10]):
            cover_image = _extract_cover_image(entry)
            liked_count = _extract_liked_count(entry)
            published = _extract_published(entry)
            description = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
            
            link = getattr(entry, 'link', '')
            note_id = _extract_note_id(link, i)
            title = getattr(entry, 'title', 'æ— æ ‡é¢˜')
            
            print(f"[DEBUG] ç¬”è®°: {title[:30]}... | å›¾ç‰‡: {cover_image[:80] if cover_image else 'æ— '}")
            
            notes.append({
                "id": note_id,
                "title": title,
                "note_url": link,
                "cover_image": cover_image,
                "description": description,
                "author": author_name,
                "author_tags": author_tags,
                "liked_count": liked_count,
                "published": published,
            })
            
    except Exception as e:
        print(f"[XHS] RSS è§£æé”™è¯¯: {e}")
    
    return notes


def _extract_note_id(link: str, index: int) -> str:
    """ä»é“¾æ¥æå–ç¬”è®° ID"""
    if '/discovery/item/' in link:
        return link.split('/discovery/item/')[-1].split('?')[0]
    elif '/explore/' in link:
        return link.split('/explore/')[-1].split('?')[0]
    return f"note_{index}"


def _extract_cover_image(entry) -> str:
    """æå–å°é¢å›¾ç‰‡"""
    content = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
    
    if not content:
        return ""
    
    # è§†é¢‘å°é¢
    video_match = re.search(r'poster=["\']([^"\']+)["\']', content)
    if video_match:
        return _proxy_image(video_match.group(1))
    
    # å›¾ç‰‡
    img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', content)
    if img_match:
        return _proxy_image(img_match.group(1))
    
    return ""


def _extract_liked_count(entry) -> int:
    """æå–ç‚¹èµæ•°"""
    # RSSHub å¯èƒ½åœ¨ upvotes å­—æ®µè¿”å›ç‚¹èµæ•°
    if hasattr(entry, 'upvotes'):
        try:
            return int(entry.upvotes)
        except (ValueError, TypeError):
            pass
    
    # å°è¯•ä»æè¿°ä¸­æå–
    content = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
    match = re.search(r'(\d+)\s*(?:èµ|ç‚¹èµ|likes?)', content, re.IGNORECASE)
    if match:
        return int(match.group(1))
    
    return 0


def _extract_published(entry) -> Optional[str]:
    """æå–å‘å¸ƒæ—¶é—´"""
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        try:
            dt = datetime(*entry.published_parsed[:6])
            return dt.strftime("%Y-%m-%d")
        except Exception:
            pass
    
    if hasattr(entry, 'published'):
        return entry.published[:10] if len(entry.published) >= 10 else entry.published
    
    return None


def _proxy_image(img_url: str) -> str:
    """ä»£ç†å›¾ç‰‡ URLï¼Œè§£å†³é˜²ç›—é“¾"""
    if not img_url:
        return ""
    encoded = urllib.parse.quote(img_url, safe='')
    return f"https://wsrv.nl/?url={encoded}"


def _filter_by_keyword(notes: List[Dict], keyword: str) -> List[Dict]:
    """æ ¹æ®å…³é”®è¯ç­›é€‰ç¬”è®°ï¼ˆæœç´¢æ ‡é¢˜å’Œæè¿°ï¼‰"""
    if not keyword:
        return notes
    
    keywords = keyword.lower().split()
    
    def match_note(note):
        title = note.get("title", "").lower()
        desc = note.get("description", "").lower()
        return any(kw in title or kw in desc for kw in keywords)
    
    matched = [note for note in notes if match_note(note)]
    print(f"[DEBUG] å…³é”®è¯ç­›é€‰: {keyword} -> åŒ¹é… {len(matched)}/{len(notes)} æ¡")
    return matched


def _build_search_url(keyword: str) -> str:
    """æ„å»ºå°çº¢ä¹¦æœç´¢ URL"""
    encoded = urllib.parse.quote(keyword)
    return f"https://www.xiaohongshu.com/search_result?keyword={encoded}"
